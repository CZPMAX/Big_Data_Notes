# ODS数据镜像层

> 在此层中主要发生数据从mysql数据库中迁移到hive数据仓库的动作
>
> 分区表在在这里创建



## 1.前言

==重要点==

在数据仓库的ODS层中，数据基本保持不变，它提供数据的临时存储服务，从mysql中导出数据，再导入到

hive数据仓库的这一步操作，是利用sqoop数据迁移工具完成的，而ods层数据转移到dwd层的操作，则是利用

insert+select语句完成的数据导入，为了有利于后期的数据查询与修改操作，我们在ods层建表时，就会采取建立

分区表的形式，后期我们的增量数据加入的时候，就是按照一个时间分区一个时间分区的形式，加入到表中。

==注意==：在从mysql导入数据之前，一定得先在hive的ods层中建立相对应的表才行，表的结构与mysql中表的结构

应一致，字段数据类型，字段对应关系应一 一对应，hive中的数据类型不仅支持sql类的数据类型，还支持java语

言的数据类型



> ----------------------------------------------------------------------------------------------------------------------------------------------------



## 2. 操作流程

> 一切项目开发都是在datagrip软件中进行的

- 连接mysql，hive数据库，新建与hive数据连接的会话 ==> console_hive (这里只是举个例子)

- 选中mysql中要操作的表 crtl + q  查看建表语句，并且复制下来

- 在console中，粘贴建表语句，修改表名与字段类型，hive中不讲主键 not null等建表的限制，所以都要去

  除，在建表的末尾加上hive表的序列化方式，指定字段之间的分割符，hive数据文件的存储格式（hive默认的

  存储格式为textfile）与采用的压缩算法等，如果有需要还能指定数据文件的存储位置

- 编写sqoop程序，使用jdbc协议让hive与mysql建立通信，采用适合的api接口，来传输数据到指定的表中，例

  如hive建表时，如不指定文件存储格式，默认为textfile这时候只要采取hive-database 与hive-table这两个api

  接口就能连接到相对应的表中，如果是orc文件类型的，则必须采取hcatalog-database 与 hcatalog-table这

  两个接口

- 接着在虚拟机端运行sqoop程序，导入数据到ods层的表中即可

- 因为ods层中表的类型是分区表，所以接下来增量导入数据，利用sqoop的query语句 按时间分区来导入数据

  ```sql
  (create_time between '${TD_DATE} 00:00:00' and '${TD_DATE} 23:59:59') or (update_time between '${TD_DATE} 00:00:00' and '${TD_DATE} 23:59:59') --筛选条件
  ```

- ods层中的表根据需求，选择合适的表来构建分区表，老师的示例中，分区表的分区字段为dt

- ods层中的表不是所有的表都要来设置分区的，像中国行政区域字典表就不用设置，需不需要设置分区表，主

  要就是看这个表中的数据以后需不需要被大量的更新与查询

==至此ods层的操作结束==



==接下来数据导入到dwd层==

使用insert+select语法将数据从ods层导入到dwd层中，这时候可以在select语句中写过滤条件，裁剪过滤字段，

过滤脏数据，完成数据的清洗