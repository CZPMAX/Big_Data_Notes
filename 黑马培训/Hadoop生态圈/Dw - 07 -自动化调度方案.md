## 自动化调度方案

==问题的引出==

![image-20221011210547540](E:\黑马培训\Hadoop生态圈\assets\image-20221011210547540.png)

## 1. 工作流调度与oozie

- workflow工作流的概念

  - 工作流（Workflow），指“==业务过程的部分或整体在计算机应用环境下的自动化==”。
  - 工作流解决的主要问题是：为了实现某个业务目标，利用计算机软件在多个参与者之间按某种预定规则自动传递文档、信息或者任务。
  - 核心概念:==依赖执行== ==周期重复执行==
  - DAG（有向无环图）

  ![image-20211207153527233](E:\黑马培训\Hadoop生态圈\assets\image-20211207153527233.png)

- Apache Oozie介绍

  - Oozie是一个用来管理 Hadoop生态圈job的工作流调度系统。由Cloudera公司贡献给Apache。

  ![image-20211207153604869](E:\黑马培训\Hadoop生态圈\assets\image-20211207153604869.png)

  - ==oozie本身单独使用极其不方便，配置文件极其繁琐==，不管是使用shell命令提交工作流还是使用java API提交工作流，都需要编写大量繁琐的xml配置文件；
  - 但是==oozie和hue整合==之后使用还是非常不错的，在hue页面上提供了拖拽功能，直接选择想要调度的脚本或者其他任务，还可以根据自己的需求编写定时规则。

- ==**栗子 1**==：在Hue上使用oozie提交一个shell脚本执行

  - step1：打开hue页面

    http://hadoop02:8889/hue    用户名、密码：hue

  - step2：上传一个shell脚本或者使用hue在线编写一个shell脚本

    ![image-20211207154319705](E:\黑马培训\Hadoop生态圈\assets\image-20211207154319705.png)

  - step3：配置oozie调度任务

    ![image-20211207154407666](E:\黑马培训\Hadoop生态圈\assets\image-20211207154407666.png)

    ![image-20211207154520342](E:\黑马培训\Hadoop生态圈\assets\image-20211207154520342.png)

    ![image-20211207154608753](E:\黑马培训\Hadoop生态圈\assets\image-20211207154608753.png)

    ![image-20211207154631536](E:\黑马培训\Hadoop生态圈\assets\image-20211207154631536.png)

    ![image-20211207154729854](E:\黑马培训\Hadoop生态圈\assets\image-20211207154729854.png)

    ![image-20211207154756388](E:\黑马培训\Hadoop生态圈\assets\image-20211207154756388.png)

![image-20211207154936254](E:\黑马培训\Hadoop生态圈\assets\image-20211207154936254.png)

![image-20211207155141788](E:\黑马培训\Hadoop生态圈\assets\image-20211207155141788.png)

![image-20211207155209422](E:\黑马培训\Hadoop生态圈\assets\image-20211207155209422.png)

- ==**栗子2**==：针对栗子1的调度任务，配置周期定时执行coordinator。

  > 刚才配置的workflow属于一次性的工作流，执行完就结束了。
  >
  > 可以==配置coordinator来控制workflow的执行周期和触发频率==。

  ![2、oozie调度--coordinator定时工作流入口](E:\黑马培训\Hadoop生态圈\assets\2、oozie调度--coordinator定时工作流入口.png)

  ![image-20211207155342016](E:\黑马培训\Hadoop生态圈\assets\image-20211207155342016.png)

  ![image-20211207155435961](E:\黑马培训\Hadoop生态圈\assets\image-20211207155435961.png)

  ![image-20211207155503337](E:\黑马培训\Hadoop生态圈\assets\image-20211207155503337.png)

  ![image-20211207155712125](E:\黑马培训\Hadoop生态圈\assets\image-20211207155712125.png)

  ![image-20211207155851433](E:\黑马培训\Hadoop生态圈\assets\image-20211207155851433.png)

  ![image-20211207155910176](E:\黑马培训\Hadoop生态圈\assets\image-20211207155910176.png)

----

> oozie提交任务流是通过mr程序提交的

## 2. shell基本知识回顾

- date命令

- ![image-20221011234625902](E:\黑马培训\Hadoop生态圈\assets\image-20221011234625902.png)

- ![image-20221011234734810](E:\黑马培训\Hadoop生态圈\assets\image-20221011234734810.png)

- ![image-20221011235206818](E:\黑马培训\Hadoop生态圈\assets\image-20221011235206818.png)

  ```shell
  #获取今天的日期
  date
  date +%Y%m%d
  
  #获取指定日期的年月日格式输出
  date -d "2014-11-12" +%Y%m%d
  
  #获取指定日期的星期（周几）格式输出
  date --date="2014-11-23" +%w
  
  #获取上周日期（day,month,year,hour）
  date -d "-1 week" +%Y%m%d
  
  #获取昨天日期
  date -d '-1 day' "+%Y-%m-%d"
  date --date="-24 hour" +%Y%m%d
  
  -- -1 day 就是过去的日期
  --  1 day 就是未来的日期
  ```

- 变量提取、反引号的功能

  ```shell
  name="allen"
  echo ${name}
  
  date
  nowTime=date
  echo ${nowTime}
  
  date
  nowTime=`date` 
  echo ${nowTime}
  ```

- 数字运算

  ```shell
  #双小括号命令是用来执行数学表达式的，可以在其中进行各种逻辑运算、数学运算，也支持更多的运算符（如++、--等）
  
  echo $(((5 * 2)))
  
  i=5
  echo $(((i=$i*2))) #10
  echo $(((i=i*2)))  #20
  
  # $((( )))的缩写。
  echo $(((i*2))) #40
  echo $((i*2)) #40
  ```

- 串行与并行

  ```shell
  #shell脚本默认是按顺序串行执行的，使用&可以将一个命令放在后台运行，从而使shell脚本能够继续往后执行
  
  sleep 5 &
  echo "done"
  
  sleep 5  #休眠5s
  echo "done"
  
  #上面的脚本执行后会立即打印出"done"，sleep命令被扔给后台执行，不会阻塞脚本执行。
  
  #如果想要在进入下个循环前，必须等待上个后台命令执行完毕，可以使用wait命令
  sleep 5 &
  wait
  echo "done"
  
  #这样，需要等待5s后才能在屏幕上看到"done"。
  ```

- shell动态传参

  > $1   $2   代表的是传递的第几个参数
  >
  > $0  执行脚本的名字
  >
  > $# 传递参数的总个数
  >
  > $* 显示所有传入参数 以列表展示

  ```shell
  [root@hadoop02 ~]# vim 4.sh
  #!/bin/bash
  echo "传递的第一个参数是：$1"
  echo "传递的第三个参数是：$3"
  echo "脚本的名称是：$0"
  echo "传递参数总个数是：$#"
  echo "所有的参数列表：$*"
  
  [root@hadoop02 ~]# sh 4.sh  11 22 33 44
  11
  33
  4.sh
  4
  11 22 33 44
  ```

----

## 3. 脚本实现、调度实现

- sqoop脚本本身就是shell脚本
- 要使用hive的语句的话 需要两个参数 -e 是执行一条sql语句 -f 是执行一个sql文件
- presto 则要安装一个客户端

- 脚本实现

  > 脚本实现的关键是如何在shell建表中执行sqoop命令、hive sql文件、presto sql文件
  >
  > 并且关于==时间==的地方==不能写死==，而是使用shell  ==date命令来动态获取==

  - 例子一：ODS数据导入

    ```shell
    #! /bin/bash
    SQOOP_HOME=/usr/bin/sqoop
    if [[ $1 == "" ]];then
       TD_DATE=`date -d '1 days ago' "+%Y-%m-%d"`
    else
       TD_DATE=$1
    fi
    
    #上述这段shell是什么意思？能否看懂？
    	如果用户不指定日期 默认采集当前天的前一天的数据。
    	用户也可以根据自己需求传入指定日期作为参数  就采集指定那一天的数据。
    ```

    - 首次执行脚本

      ```shell
      #仅新增
      /usr/bin/sqoop import "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" \
      --connect 'jdbc:mysql://172.17.0.202:3306/yipin?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true' \
      --username root \
      --password 123456 \
      --query "select *, '${TD_DATE}' as dt from t_goods_evaluation where 1=1 \$CONDITIONS" \
      --hcatalog-database yp_ods \
      --hcatalog-table t_goods_evaluation \
      -m 1
      
      # 新增和更新同步
      /usr/bin/sqoop import "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" \
      --connect 'jdbc:mysql://172.17.0.202:3306/yipin?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true' \
      --username root \
      --password 123456 \
      --query "select *, '${TD_DATE}' as dt from t_store where 1=1 and \$CONDITIONS" \
      --hcatalog-database yp_ods \
      --hcatalog-table t_store \
      -m 1
      ```

    - 循环执行脚本

      > 1、通过sqoop的query查询把增量数据查询出来。
      >
      > 增量的范围是TD_DATE值的 00：00：00  至  23：59：59
      >
      > 2、判断的字段是
      >
      > ​	如果是仅新增同步，使用create_time创建时间即可
      >
      > ​	如果是新增和更新同步，需要使用create_time 和 update_time两个时间
      >
      > 3、这也要求在业务系统数据库设计的时候，需要有意识的增加如下字段
      >
      > ​	create_user
      >
      > ​	create_time
      >
      > ​	update_user
      >
      > ​	update_time

      ```shell
      #仅新增
      /usr/bin/sqoop import "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" \
      --connect 'jdbc:mysql://172.17.0.202:3306/yipin?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true' \
      --username root \
      --password 123456 \
      --query "select *, '${TD_DATE}' as dt from t_goods_evaluation where 1=1 and (create_time between '${TD_DATE} 00:00:00' and '${TD_DATE} 23:59:59') and  \$CONDITIONS" \
      --hcatalog-database yp_ods \
      --hcatalog-table t_goods_evaluation \
      -m 1
      
      # 新增和更新同步
      /usr/bin/sqoop import "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" \
      --connect 'jdbc:mysql://172.17.0.202:3306/yipin?useUnicode=true&characterEncoding=UTF-8&autoReconnect=true' \
      --username root \
      --password 123456 \
      --query "select *, '${TD_DATE}' as dt from t_store where 1=1 and ((create_time between '${TD_DATE} 00:00:00' and '${TD_DATE} 23:59:59') or (update_time between '${TD_DATE} 00:00:00' and '${TD_DATE} 23:59:59')) and  \$CONDITIONS" \
      --hcatalog-database yp_ods \
      --hcatalog-table t_store \
      -m 1
      ```

  - 例子二：DWB层sql脚本执行

    > 在shell中执行hive sql的方式有两种
    >
    > ==bin/hive -e ‘sql语句’==
    >
    > ==bin/hive -f  xxx.sql文件==

    - 首次执行

      ```shell
      #! /bin/bash
      HIVE_HOME=/usr/bin/hive
      
      
      ${HIVE_HOME} -S -e "
      --分区
      SET hive.exec.dynamic.partition=true;
      SET hive.exec.dynamic.partition.mode=nonstrict;
      set hive.exec.max.dynamic.partitions.pernode=10000;
      set hive.exec.max.dynamic.partitions=100000;
      set hive.exec.max.created.files=150000;
      --=======订单宽表=======
      insert into yp_dwb.dwb_order_detail partition (dt)
      select
      	xxxxxxx
      ;
      ```

    - 循环执行

      ```shell
      #! /bin/bash
      HIVE_HOME=/usr/bin/hive
      
      #上个月1日
      Last_Month_DATE=$(date -d "-1 month" +%Y-%m-01)
      
      
      ${HIVE_HOME} -S -e "
      --分区配置
      SET hive.exec.dynamic.partition=true;
      SET hive.exec.dynamic.partition.mode=nonstrict;
      set hive.exec.max.dynamic.partitions.pernode=10000;
      set hive.exec.max.dynamic.partitions=100000;
      set hive.exec.max.created.files=150000;
      
      --=======订单宽表=======
      --增量插入
      insert overwrite table yp_dwb.dwb_order_detail partition (dt)
      select
      	xxxxxx
      -- 读取上个月1日至今的数据
      SUBSTRING(o.create_date,1,10) >= '${Last_Month_DATE}' and o.start_date >= '${Last_Month_DATE}';
      ```

  - 例子三：Presto的sql如何在shell中执行

    ```shell
    #! /bin/bash
    #昨天
    if [[ $1 == "" ]];then
       TD_DATE=`date -d '1 days ago' "+%Y-%m-%d"`
    else
       TD_DATE=$1
    fi
    
    PRESTO_HOME=/opt/cloudera/parcels/presto/bin/presto
    
    
    ${PRESTO_HOME} --catalog hive --server 172.17.0.202:8090 --execute "
    delete from yp_rpt.rpt_sale_store_cnt_month where date_time = '${TD_DATE}';
    .......
    "
    
    
    #改写上述的模板 练习一下如何使用shell执行presto sql
    
    /export/server/presto/bin/presto --catalog hive --server hadoop01:8090 --execute "select * from yp_rpt.rpt_sale_store_cnt_month where date_time = '2021-03-17'"
    ```

- 调度实现

  > 脚本可以参考课程资料。
